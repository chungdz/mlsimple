# Description
The source code is for click prediction model experiments.

# Package Introduction
- gscope: the code for generate scope scripts to fetch dataset.
- nn_modules: the code for neural network in Pytorch.
- prepocess: the code for processing dataset.
- utils: the code for tools.

# Prepare Data
Data is stored in DLTS Azure-EastUS-P40-2 /data/yunfanhu/. They are first generated in Cosmos and then moved to DLTS. Since I generate the data day by day. The final files for training and validation are concatenated in DLTS. Training datasets are from 2022/03/24 to 2022/04/06. Validation datasets are from 2022/04/07/ to 2022/04/13.

Original data: Combine counting features and id features. It is stored in Cosmos and never moved to DLTS. The scope scripts to make the original data are generated by original_data.ipynb in gscope folder.

D1: Dataset uniformly sampled from original data and sample rate is 6%. It is stored in /data/yunfanhu/samples/ as train.tsv and valid.tsv. Use valid_3M.tsv to do validation. The scope scripts to make the original data are generated by D1.ipynb in gscope folder.

D2: Dataset that has 5 million rows for training and 1 million rows for validation. It is used for fast experiments. It is stored in /data/yunfanhu/samples/ as train_5M.tsv and valid_1M.tsv. It is sampled from D1.

D3: Dataset uniformly sampled from original data and sample rate is 20%. It is stored in /data/yunfanhu/samples_20/ as train.tsv and valid.tsv. Use valid_5M.tsv to do validation. The scope scripts to make the original data are generated by D3.ipynb in gscope folder.

D4: D1 adding UGE embeddings. It is stored in /data/yunfanhu/samples_emb/ as train.tsv and valid.tsv. Use valid_5M.tsv to do validation. The scope scripts to make the original data are generated by D4.ipynb in gscope folder.

D5: Dataset has all positive samples from original dataset and downsampled negative sample. The number of rows of D5 is 20% of total. It is stored in /data/yunfanhu/downsamples_20/ as train.tsv and valid.tsv. Use valid_5M.tsv to do validation. The scope scripts to make the original data are generated by D5.ipynb in gscope folder.

# Set Environments
Python version should be equal or larger than 3.8. Currently Python 3.9 is used.

Pytorch=1.12.1 is needed.

Instruction to install other packages.
```shell
pip install datasets transformers tensorboard sklearn lightgbm matplotlib ipython
``` 

# prepare the runnable dataset
The dataset downloaded from website cannot be used currently, and need to be processed first.

```shell
chmod -R 700 run.sh
./run.sh
``` 

run.sh assumes 4 GPUs will be used, if the GPU number is different, change the processes argument in the code:

```
python -m prepocess.convert_train --processes=#GPUs
```
Number of GPUs can be 1, 2, 4, 8. 

# Training and testing
To train the model, run:
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 python training.py --gpus=4 --epoch=10
```

Based on the validation results, choose parameters from one epoch (i.e. epoch 3) to do the test:
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 python validate.py --gpus=4 --epoch=3 --filenum=20
```
In these instructions, argument gpus means the number of GPUs. 

# Experiment Environment
This model was trained and tested by using 4 Tesla P40 GPUs. The memory of the device was 128GB.