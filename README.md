# Description
The source code is for click prediction model experiments.

# Package Introduction
- gscope: the code for generate scope scripts to fetch dataset.
- nn_modules: the code for neural network in Pytorch.
- prepocess: the code for processing dataset.
- utils: the code for tools.

# Prepare Data
Data is stored in DLTS Azure-EastUS-P40-2 /data/yunfanhu/. They are first generated in Cosmos and then moved to DLTS. Since I generate the data day by day. The final files for training and validation are concatenated in DLTS. Training datasets are from 2022/03/24 to 2022/04/06. Validation datasets are from 2022/04/07/ to 2022/04/13.

Original data: Combine counting features and id features. It is stored in Cosmos and never moved to DLTS. The scope scripts to make the original data are generated by original_data.ipynb in gscope folder.

D1: Dataset uniformly sampled from original data and sample rate is 6%. It is stored in /data/yunfanhu/samples/ as train.tsv and valid.tsv. Use valid_3M.tsv to do validation. The scope scripts to make the original data are generated by D1.ipynb in gscope folder.

D2: Dataset that has 5 million rows for training and 1 million rows for validation. It is used for fast experiments. It is stored in /data/yunfanhu/samples/ as train_5M.tsv and valid_1M.tsv. It is sampled from D1.

D3: Dataset uniformly sampled from original data and sample rate is 20%. It is stored in /data/yunfanhu/samples_20/ as train.tsv and valid.tsv. Use valid_5M.tsv to do validation. The scope scripts to make the original data are generated by D3.ipynb in gscope folder.

D4: D1 adding UGE embeddings. It is stored in /data/yunfanhu/samples_emb/ as train.tsv and valid.tsv. Use valid_5M.tsv to do validation. The scope scripts to make the original data are generated by D4.ipynb in gscope folder.

D5: Dataset has all positive samples from original dataset and downsampled negative sample. The number of rows of D5 is 20% of total. It is stored in /data/yunfanhu/downsamples_20/ as train.tsv and valid.tsv. Use valid_5M.tsv to do validation. The scope scripts to make the original data are generated by D5.ipynb in gscope folder.

# Set Environments
Python version should be equal or larger than 3.8. Currently Python 3.9 is used.

Pytorch=1.12.1 is needed.

Instruction to install other packages.
```shell
pip install datasets transformers tensorboard sklearn lightgbm matplotlib ipython
``` 

# Get meta infomation of different features
Before train the model, meta infomation should be gathered to help transform the data. 

Min and max of the counting features are collected. It is better than mean and standard diviation for normalizing the counting feature. First, min and max are easy to collect and do not need to calculate. Second, for features only have zero and one, the normalized featrue stays the same. Third, for features has only one unique number, it avoids number divided by zero.

The frequencies of each unique ID are gathered. The IDs appears less than threshold are map to *Unknown*.

The meta information is stored as Json format and in the same folder of the train.tsv.

For dataset without UGE embeddings, the example instructions are:

```shell
python -m preprocess.get_meta --dpath=/data/yunfanhu/samples_20 \
                            --filep=train.tsv \
                            --vfilep=valid.tsv \
                            --chunk_size=50000

python -m preprocess.process_meta --dpath=/data/yunfanhu/samples_20 \
                                    --drop_num=10
``` 

The meaning of each argument can be found in python code or use *-h*

For dataset with UGE embeddings, the example instructions are:

```
python -m preprocess.get_meta_emb --dpath=/data/yunfanhu/samples_emb \
                                --filep=train.tsv \
                                --vfilep=valid.tsv \
                                --chunk_size=50000
python -m preprocess.process_meta --dpath=/data/yunfanhu/samples_emb \
                                    --drop_num=10

```

The meaning of each argument can be found in python code or use *-h*

# Training and testing
To train the model without UGE embeddings two codes can be run:

```shell
python train.py --dpath=/data/yunfanhu/samples_20 \
                    --batch_size=2 \
                    --chunk_size=2048 \
                    --filep=train.tsv \
                    --vfilep=valid_5M.tsv \
                    --max_steps=300000 \
                    --save_path=cps_20 \
                    --plots=plots/m1_20.jpg \
                    --save_steps=30000
```

The meaning of each argument can be found in python code or use *-h*. This code iteratively fetch data chunk from disk to avoid memory problem when dataset is too large. The chunk size can not be too large or too small. Either case makes GPUs hungry. The arguments related to steps need to be calculate before training start. 

For example, the D3 dataset has 543886254 rows. With number of GPUs in environment is 4, batch size is 2, and chunk size is 2048, the rows consumed by one step is 4 * 2 * 2048 = 16384. 

Based on the validation results, choose parameters from one epoch (i.e. epoch 3) to do the test:
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 python validate.py --gpus=4 --epoch=3 --filenum=20
```
In these instructions, argument gpus means the number of GPUs. 

# Experiment Environment
This model was trained and tested by using 4 Tesla P40 GPUs. The memory of the device was 128GB.